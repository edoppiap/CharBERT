{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary instructions and installation of dependencies"
      ],
      "metadata": {
        "id": "bpQvo0P6oP5p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_W54jMD47Kj",
        "outputId": "86afcfc2-5e32-4bff-a840-9bf7912a3180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CharBERT'...\n",
            "remote: Enumerating objects: 362, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 362 (delta 91), reused 117 (delta 48), pack-reused 189\u001b[K\n",
            "Receiving objects: 100% (362/362), 3.32 MiB | 13.26 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/edoppiap/CharBERT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT5PXlQgEF-N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49a7af2-4d4e-4eaa-e53e-df5f0f4f3b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7LwNdVA-FLQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43501858-747c-4983-aa3d-195b38f4600a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CharBERT\n"
          ]
        }
      ],
      "source": [
        "cd /content/CharBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMZneKwzjoYs"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install --q boto3 GitPython langchain chromadb sentence_transformers\n",
        "%pip install langchain_community==0.0.16\n",
        "!pip -q install google-generativeai==0.3.1\n",
        "!pip -q install google-ai-generativelanguage==0.4.0\n",
        "!pip -q install langchain-google-genai\n",
        "!pip install langchain==0.1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxzT41wtjrjS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from git import Repo\n",
        "from google.colab import userdata\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import LanguageParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.text_splitter import Language\n",
        "\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "\n",
        "from modeling.modeling_charbert import CharBertTransformer\n",
        "from modeling.charbert_embeddings import CharBertEmbeddings\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import GoogleGenerativeAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwmxWvM_AMdr"
      },
      "source": [
        "## Download and parse the Github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAsgN78-AMV4"
      },
      "outputs": [],
      "source": [
        "# Clone a github repo\n",
        "repo_path = \"/content/CharBERT/db_charbert\"\n",
        "#github_repo = \"https://github.com/mawentao277/CharBERT\" ## any github repository URL\n",
        "github_repo = 'https://github.com/edoppiap/casl_labs'\n",
        "repo = Repo.clone_from(github_repo, to_path=repo_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwLXrR9LAL8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42a24e2-ec1c-4fe2-fba5-13d1348c0bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Load\n",
        "loader = GenericLoader.from_filesystem(\n",
        "    #repo_path + \"/libs/langchain/langchain\",\n",
        "    repo_path,\n",
        "    glob=\"**/*\",\n",
        "    suffixes=[\".py\"],\n",
        "    #exclude=[\"**/non-utf8-encoding.py\"],\n",
        "    parser=LanguageParser(language=Language.PYTHON),\n",
        ")\n",
        "documents = loader.load()\n",
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r'(?:def|class)\\s+(\\w+)\\s*'\n",
        "\n",
        "i_to_del = []\n",
        "\"\"\"for i, doc in enumerate(documents):\n",
        "    matches = re.findall(pattern, doc.page_content)\n",
        "    doc.metadata['wrap_name'] = doc.metadata['source'] + '_' + matches[0]\n",
        "    if doc.metadata['content_type'] == 'simplified_code':\n",
        "        i_to_del.append(i)\"\"\"\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "\n",
        "    if 'content_type' in doc.metadata:\n",
        "        matches = re.findall(pattern, doc.page_content)\n",
        "\n",
        "        if len(matches) == 0:\n",
        "            i_to_del.append(i)\n",
        "\n",
        "        else:\n",
        "            doc.metadata['wrap_name'] = doc.metadata['source'] + '_' + matches[0]\n",
        "            if doc.metadata['content_type'] == 'simplified_code':\n",
        "                i_to_del.append(i)\n",
        "\n",
        "    else:\n",
        "        i_to_del.append(i)\n",
        "\n",
        "for ix in reversed(i_to_del):\n",
        "    documents.pop(ix)\n",
        "\n",
        "for doc in documents:\n",
        "    print(doc.metadata)"
      ],
      "metadata": {
        "id": "X7yHDrl3GV6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjOWKDD0BnlS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16b7b885-fa8b-4c81-b654-f90bab3b6875"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "261"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
        "    language=Language.PYTHON, chunk_size=1000, chunk_overlap=0\n",
        ")\n",
        "texts = python_splitter.split_documents(documents)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inizialize CharBert model for Sentence Embeddings"
      ],
      "metadata": {
        "id": "kx_6Gn-Gpa48"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVqeqw8x-HX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bc6428-accf-49c1-9dfd-e03cd32b8c91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cls: <class 'modeling.configuration_bert.BertConfig'>\n",
            "pretrained_model_name_or_path: /content/drive/MyDrive/NLP_Project/CharBERT/charbert-bert-wiki\n",
            "cls.pretrained_config_archive_map: {'bert-base-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json', 'bert-large-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json', 'bert-base-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json', 'bert-large-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json', 'bert-base-multilingual-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json', 'bert-base-multilingual-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json', 'bert-base-chinese': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json', 'bert-base-german-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json', 'bert-large-uncased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json', 'bert-large-cased-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json', 'bert-large-uncased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json', 'bert-large-cased-whole-word-masking-finetuned-squad': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json', 'bert-base-cased-finetuned-mrpc': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json', 'bert-base-german-dbmdz-cased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json', 'bert-base-german-dbmdz-uncased': 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json', 'bert-base-japanese': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-config.json', 'bert-base-japanese-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-whole-word-masking-config.json', 'bert-base-japanese-char': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-config.json', 'bert-base-japanese-char-whole-word-masking': 'https://s3.amazonaws.com/models.huggingface.co/bert/cl-tohoku/bert-base-japanese-char-whole-word-masking-config.json', 'bert-base-finnish-cased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-cased-v1/config.json', 'bert-base-finnish-uncased-v1': 'https://s3.amazonaws.com/models.huggingface.co/bert/TurkuNLP/bert-base-finnish-uncased-v1/config.json'}\n"
          ]
        }
      ],
      "source": [
        "charBertTransformer = CharBertTransformer(model_type = 'bert',\n",
        "                                          model_name_or_path = '/content/drive/MyDrive/NLP_Project/CharBERT/charbert-bert-wiki', ## download it from the link in the readme\n",
        "                                          char_vocab = '/content/CharBERT/data/dict/bert_char_vocab')\n",
        "\n",
        "pooling_model = models.Pooling(charBertTransformer.get_word_embedding_dimension()*2)\n",
        "embeddings = SentenceTransformer(modules=[charBertTransformer, pooling_model])\n",
        "sentenceEmbeddings = CharBertEmbeddings(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a chroma vector database given the github repository"
      ],
      "metadata": {
        "id": "hRPmlDCFpyGi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgqXAo4-_B4n"
      },
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(texts , sentenceEmbeddings, persist_directory='./chroma_db')\n",
        "\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\",  # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 6},\n",
        ")\n",
        "db.persist()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load an existing vector database"
      ],
      "metadata": {
        "id": "sVkrRdHDqe0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJQ2p0MwW2D6"
      },
      "outputs": [],
      "source": [
        "db = Chroma(persist_directory='/content/drive/MyDrive/NLP_Project/db/casl_ema_chroma_db_giusto', embedding_function=sentenceEmbeddings)\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"mmr\",  # Also test \"similarity\"\n",
        "    search_kwargs={\"k\": 10},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize Google gemini environment and LLM"
      ],
      "metadata": {
        "id": "aAhNEjJlrEIQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX1jNEVXWM5l"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDVRweStnbEJUjEAV9Mah2ZhEUp2kz0w2M\"\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "llm = GoogleGenerativeAI(model=\"models/gemini-1.0-pro-001\")\n",
        "llm_latest = GoogleGenerativeAI(model=\"models/gemini-1.0-pro-latest\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpJhC_23WTjc"
      },
      "source": [
        "#RAG MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VDMzXuAWdxN"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "from IPython.display import display\n",
        "\n",
        "template = \"\"\"\n",
        "You are an AI assistant that helps users understand Github repositories\n",
        "that are provided to you via context input,\n",
        "the context you are given is composed by python scripts of the repository: {repo} |\n",
        "This documents are the context input Docs: {context} |\n",
        "This is the question you are going to aswer: {question}\n",
        "\n",
        "Instructions:\n",
        "1. Answer based on the documents given in the context, use only the functions seen in the context.\n",
        "2. Focus on repo/code.\n",
        "3. Consider:\n",
        "    a. Purpose/features - describe.\n",
        "    b. Functions/code - provide details/samples.\n",
        "    c. Setup/usage - give instructions.\n",
        "4. Unsure? Say \"I am not sure\".\n",
        "5. Tell me if you used the context to answer the question.\n",
        "6. Tell me from which documents/function_classes in the context you took the answer using the index of the documents in the list of documents.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"repo\",\"question\", \"context\"]\n",
        ")\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "def retrieve_documents(question=None, retriever=None, llm=None, what_retrieve='chunks', transform_in_code=False):\n",
        "  if question == None:\n",
        "    print('Need a question to retrieve documents')\n",
        "    return None\n",
        "\n",
        "  if retriever == None:\n",
        "    print('Need retriever to retrieve documents')\n",
        "    return None\n",
        "\n",
        "  if what_retrieve == 'functions_or_classes':\n",
        "    if transform_in_code:\n",
        "      if llm:\n",
        "        code_query = get_code_from_question(question, llm)\n",
        "        print(f\"This is the code query generated from the LLM:\\n {code_query}\\n----------------------\\n\")\n",
        "      documents = retriever.get_relevant_documents(code_query)\n",
        "    else:\n",
        "      documents = retriever.get_relevant_documents(question)\n",
        "    metadata = []\n",
        "    for d in documents:\n",
        "      metadata.append(d.metadata['wrap_name'])\n",
        "    metadata = set(metadata)\n",
        "    full_res = []\n",
        "    for wrap_name in metadata:\n",
        "      res = ''\n",
        "      document_list = db.get(where={\"wrap_name\": wrap_name})['documents']\n",
        "      for s in document_list:\n",
        "        res += s\n",
        "      full_res.append(res)\n",
        "    function_or_classes_list = []\n",
        "    for i, function_class_content in enumerate(full_res):\n",
        "      function_or_classes_list.append(f'Start function/class {i}: \\n'+function_class_content+f'\\nEnd function/class {i}')\n",
        "    return function_or_classes_list\n",
        "\n",
        "  elif what_retrieve == 'chunks':\n",
        "    if transform_in_code:\n",
        "      code_query = get_code_from_question(question, llm)\n",
        "      print(f\"This is the code query generated from the LLM:\\n {code_query}\\n----------------------\\n\")\n",
        "      retrieved_documents = retriever.get_relevant_documents(code_query)\n",
        "    else:\n",
        "      retrieved_documents = retriever.get_relevant_documents(question)\n",
        "  elif what_retrieve == 'entire_document':\n",
        "    if transform_in_code:\n",
        "      if llm:\n",
        "        code_query = get_code_from_question(question, llm)\n",
        "        print(f\"This is the code query generated from the LLM:\\n {code_query}\\n----------------------\\n\")\n",
        "      documents = retriever.get_relevant_documents(code_query)\n",
        "    else:\n",
        "      documents = retriever.get_relevant_documents(question)\n",
        "    metadata = []\n",
        "    for d in documents:\n",
        "      metadata.append(d.metadata['source'])\n",
        "    metadata = set(metadata)\n",
        "    document_list = []\n",
        "    for i, source in enumerate(metadata):\n",
        "      with open(source, 'r') as file:\n",
        "        file_contents = file.read()\n",
        "      document_list.append(f'Start Document {i}: \\n'+file_contents+f'\\nEnd Document {i}')\n",
        "    return document_list\n",
        "  return retrieved_documents\n",
        "\n",
        "\n",
        "def get_code_from_question(query, llm):\n",
        "  context = \"\"\"You are an AI assiantant that converts natural language in python code, return only code in your answer. This is the question: \"\"\"\n",
        "  result = llm.invoke(context+query)\n",
        "  if \"```python\\n\" in result :\n",
        "    return result.replace(\"```python\\n\", \"\").replace(\"\\n```\",\"\").replace(\"```python \\n\",\"\").replace(\"```python  \\n\",\"\")\n",
        "  else:\n",
        "    return query\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PyOapRXaD5d"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "stuff_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False)\n",
        "stuff_chain_latest = load_qa_chain(llm_latest, chain_type=\"stuff\", prompt=prompt, verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform_in_code = False\n",
        "what_retrieve = 'chunks' ## entire_document, chunks, oppure functions_or_classes\n",
        "retriever = db.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 25},) if what_retrieve == 'chunks' else db.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 6},)\n",
        "\n",
        "question = \"what is this repo about??\"\n",
        "\n",
        "retrieved_documents = retrieve_documents(question=question, retriever=retriever, llm=llm, what_retrieve=what_retrieve, transform_in_code=transform_in_code)\n",
        "\n",
        "\n",
        "if what_retrieve == 'chunks':\n",
        "  stuff_answer = stuff_chain.invoke(\n",
        "    {\"repo\": repo, \"question\": question, \"input_documents\": retrieved_documents}, return_only_outputs=True)\n",
        "  stuff_answer_latest = stuff_chain_latest.invoke(\n",
        "    {\"repo\": repo, \"question\": question, \"input_documents\": retrieved_documents}, return_only_outputs=True)\n",
        "  display(to_markdown(stuff_answer['output_text']))\n",
        "  print('\\n\\n')\n",
        "  print('_______________________________________')\n",
        "  display(to_markdown(stuff_answer_latest['output_text']))\n",
        "else:\n",
        "  result = llm.invoke(template.format(repo = repo, context = retrieved_documents, question = question))\n",
        "  result_latest = llm_latest.invoke(template.format(repo = repo, context = retrieved_documents, question = question))\n",
        "  display(to_markdown(result))\n",
        "  print('\\n\\n----------------------------------\\n\\n')\n",
        "  display(to_markdown(result_latest))"
      ],
      "metadata": {
        "id": "wlXv1EopLJnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Chat with a GitHub repo\n",
        "#repo_url = \"repo url...\" # @param {type:\"string\"}\n",
        "\n",
        "transform_in_code = False #@param {type: \"boolean\"}\n",
        "what_retrieve = \"chunks\" # @param [\"chunks\", \"entire_document\", \"functions_or_classes\"]\n",
        "question = \"what this repo is about\" # @param {type:\"string\"}\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 20},) if what_retrieve == 'chunks' else db.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": 6},)\n",
        "retrieved_documents = retrieve_documents(question=question, retriever=retriever, llm=llm, what_retrieve=what_retrieve, transform_in_code=transform_in_code)\n",
        "\n",
        "if not transform_in_code:\n",
        "  print(f'This is the query : {question}\\n----------------------\\n')\n",
        "\n",
        "if what_retrieve == 'chunks':\n",
        "  stuff_answer = stuff_chain.invoke(\n",
        "    {\"repo\": repo, \"question\": question, \"input_documents\": retrieved_documents}, return_only_outputs=True)\n",
        "  stuff_answer_latest = stuff_chain_latest.invoke(\n",
        "    {\"repo\": repo, \"question\": question, \"input_documents\": retrieved_documents}, return_only_outputs=True)\n",
        "  display(to_markdown(stuff_answer['output_text']))\n",
        "  print('\\n\\n')\n",
        "  print('_______________________________________')\n",
        "  display(to_markdown(stuff_answer_latest['output_text']))\n",
        "else:\n",
        "  result = llm.invoke(template.format(repo = repo, context = retrieved_documents, question = question))\n",
        "  result_latest = llm_latest.invoke(template.format(repo = repo, context = retrieved_documents, question = question))\n",
        "  display(to_markdown(result))\n",
        "  print('\\n\\n----------------------------------\\n\\n')\n",
        "  display(to_markdown(result_latest))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "id": "uCOhmJaeE2qi",
        "outputId": "515e26f0-1822-47fa-a7bd-3712fc31be18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the query : what this repo is about\n",
            "----------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> This repository simulates the arrival and departure of clients in a service system. It includes classes for clients, servers, and events, as well as functions for scheduling arrivals and departures.\n> \n> The main function of the repository is to simulate the behavior of a queueing system. The simulation starts with a certain number of servers and a certain arrival rate. Clients arrive at the system according to a Poisson distribution, and they are served by the servers according to a negative exponential distribution. The simulation ends when a certain number of clients have been served.\n> \n> The repository includes a number of functions for scheduling arrivals and departures. The `arrival()` function schedules the arrival of a new client at a certain time. The `departure()` function schedules the departure of a client who has been served.\n> \n> The repository also includes a number of classes for representing clients, servers, and events. The `Client` class represents a client who arrives at the system. The `Server` class represents a server who serves clients. The `Event` class represents an event that occurs in the system, such as an arrival or a departure.\n> \n> To use the repository, you can create a new instance of the `Simulation` class. You can then call the `run()` method to start the simulation. The simulation will run until a certain number of clients have been served.\n> \n> The repository is useful for simulating the behavior of a queueing system. It can be used to evaluate the performance of different queueing systems, and to identify potential bottlenecks.\n> \n> I used the context to answer the question. I used the following documents/function_classes:\n> \n> * Document 1: `Event` class\n> * Document 2: `arrival()` function\n> * Document 3: `departure()` function\n> * Document 4: `Client` class\n> * Document 5: `Server` class\n> * Document 6: `Simulation` class"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "_______________________________________\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "> a. This repository contains code for simulating a queueing system with multiple servers and different urgency levels for clients.\n> The simulation includes events such as arrivals, departures, and service completions.\n> The code keeps track of various metrics, such as the number of arrivals, the average utilization of the servers, and the number of clients in the queue and paused.\n> The simulation also includes a mechanism for adjusting the service time based on the urgency of the client.\n> \n> b. The code includes the following functions:\n> \n> - `arrival()`: This function generates a new arrival event and schedules it in the future event set (FES).\n> - `departure()`: This function processes a departure event, removing the client from the queue or paused list and updating the statistics.\n> - `start_service_if_possible()`: This function checks if there is a server available to serve a client. If so, it starts the service and schedules the end of service event. If not, it adds the client to the queue.\n> - `calculate_service_time()`: This function calculates the service time for a given client, taking into account the urgency of the client.\n> - `is_early_than()`: This function compares two events and returns True if the first event is earlier than the second event.\n> \n> c. To set up and use the simulation, you need to:\n> \n> 1. Import the necessary modules.\n> 2. Create a `Simulation` object.\n> 3. Set the parameters of the simulation, such as the arrival rate, the service rate, and the number of servers.\n> 4. Run the simulation by calling the `run()` method.\n> 5. Collect and analyze the results of the simulation.\n> \n> I used the context to answer the question.\n> I took the answer from the following documents/function_classes in the context:\n> \n> - Document 1: `arrival()`, `departure()`, `start_service_if_possible()`, `calculate_service_time()`, `is_early_than()`\n> - Document 2: `Simulation` class"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}